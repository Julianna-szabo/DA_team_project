## group by countries
ggplot( bpp, aes( x = price_online, y = price, color = country ))+
stat_summary_bin(fun='mean', binwidth = 50, geom = 'point',size = 2)+
labs( x= 'price online' , y = 'price offline', color = 'country')+
facet_wrap(~country, scales = 'free', ncol = 2) +
theme(legend.position = 'none')+ # to remove the legend
geom_smooth(method = 'lm', formula = y~x)
## group by countries
ggplot( bpp, aes( x = price_online, y = price, color = country ))+
stat_summary_bin(fun='mean', binwidth = 50, geom = 'point',size = 2)+
labs( x= 'price online' , y = 'price offline', color = 'country')+
facet_wrap(~country, scales = 'free', ncol = 2) +
theme(legend.position = 'none')+ # to remove the legend
geom_smooth(method = 'lm', formula = y~x)
## Start script
# Remove variables from the memory
rm(list=ls())
# Call packages
library(tidyverse)
library(moments)
# new package we use
#install.packages("readxl")
library(readxl)
## Import data
data_in <- "C:/Users/mbrae/OneDrive/Bureau/ECBS-5208-Coding-1-Business-Analytics-master/Class_4/data/"
# Reading with excel and telling that the parsing should check the first 40000 observations
bpp_orig <- read_excel( paste0( data_in , "raw/online_offline_ALL_clean.xls" ), guess_max = 40000 )
# Check the variables
glimpse(bpp_orig)
## Create our key variable
bpp_orig <- mutate( bpp_orig , p_diff = price_online - price )
## Check some of the key variables -> introducing the %>%
# Note: we will learn how to do this without writing this part down 3 times...
# 1) Online and offline prices
p1_sum <- bpp_orig %>% summarise(
mean     = mean(price),
median   = median(price),
std      = sd(price),
iq_range = IQR(price),
min      = min(price),
max      = max(price),
skew     = skewness(price),
numObs   = sum( !is.na( price ) ) )
p2_sum <- bpp_orig %>% summarise(
mean     = mean(price_online),
median   = median(price_online),
std      = sd(price_online),
iq_range = IQR(price_online),
min      = min(price_online),
max      = max(price_online),
skew     = skewness(price_online),
numObs   = sum( !is.na( price_online ) ) )
p3_sum <- bpp_orig %>% summarise(
mean     = mean(p_diff),
median   = median(p_diff),
std      = sd(p_diff),
iq_range = IQR(p_diff),
min      = min(p_diff),
max      = max(p_diff),
skew     = skewness(p_diff),
numObs   = sum( !is.na( p_diff ) ) )
# Join the to table
price_summary <- p1_sum %>% add_row( p2_sum ) %>% add_row( p3_sum )
price_summary
rm( p1_sum , p2_sum , p3_sum )
# Check for extreme values
# Histogram
ggplot( data = bpp_orig ) +
geom_histogram( aes( x = price ) , color = 'blue'  , alpha = 0.1 ) +
labs(x = "Price",
y = "Count" )
# Need to filter out some data
# FILTER DATA -> filter for "PRICETYPE" is a large restriction!
#     may check without that filter!
bpp <- bpp_orig %>%
filter(is.na(sale_online)) %>%
filter(!is.na(price)) %>%
filter(!is.na(price_online)) %>%
filter(PRICETYPE == "Regular Price")
# Drop obvious errors
bpp <- bpp %>%
filter( price < 1000 )
# Check the summary stat
p1_sum <- bpp %>% summarise(
mean     = mean(price),
median   = median(price),
std      = sd(price),
iq_range = IQR(price),
min      = min(price),
max      = max(price),
skew     = skewness(price),
numObs   = sum( !is.na( price ) ) )
p2_sum <- bpp %>% summarise(
mean     = mean(price_online),
median   = median(price_online),
std      = sd(price_online),
iq_range = IQR(price_online),
min      = min(price_online),
max      = max(price_online),
skew     = skewness(price_online),
numObs   = sum( !is.na( price_online ) ) )
p3_sum <- bpp %>% summarise(
mean     = mean(p_diff),
median   = median(p_diff),
std      = sd(p_diff),
iq_range = IQR(p_diff),
min      = min(p_diff),
max      = max(p_diff),
skew     = skewness(p_diff),
numObs   = sum( !is.na( p_diff ) ) )
# Join the to table
price_summary <- p1_sum %>% add_row( p2_sum ) %>% add_row( p3_sum )
price_summary
rm( p1_sum , p2_sum , p3_sum )
# Histogram
ggplot( data = bpp ) +
geom_density( aes( x = price ) , color = 'blue'  , alpha = 0.1 ) +
geom_density( aes( x = price_online )  , color = 'red' , alpha = 0.1 ) +
labs(x = "Price",
y = "Relative Frequency" )
# Check the price differences
ggplot( data = bpp ) +
geom_density( aes( x = p_diff ) , color = 'blue'  , alpha = 0.1 ) +
labs(x = "Price differences",
y = "Relative Frequency" )+
xlim(-4,4)
# Check for price differences
chck <- bpp %>% filter( p_diff > 500 | p_diff < -500 )
# Drop them
bpp <- bpp %>% filter( p_diff < 500 & p_diff > -500 )
rm( chck )
#####
## Creating factors in R
# tell R that they are nominal qualitative data
bpp$country <- factor( bpp$COUNTRY)
table (bpp$country)
# calculate the mean for each country
bpp %>% select (country, p_diff) %>%
group_by(country) %>%
summarize( mean = mean(p_diff),
sd =sd(p_diff),
num_obs= n() )
# Create ggplot for countries : histogram
ggplot(data = bpp, aes (x = p_diff, fill = country)) +
geom_histogram(aes(y=..density..), alpha= 0.4)+
labs(x= 'Price', y = 'Relative frenquncy', fill='country')+
facet_wrap(~country)+
xlim(-4,4)
#Hw : do the same with impute variable :
#1) recode it as a string
#a) if NA-> 'yes'
#b) if 1 -> 'no'
# 2) call factor variable as 'same_day'
# 3) create a summary by countries and same_day
# test HO : the average price difference
#           beween price_onine - Pice = 0
# HA : the avg price diff is non 0.
t.test (bpp$p_diff, mu = 0)
# test 2 : the online prices are smaller or equal to offline price
# HO : Price_online - price <= 0
# HA : price_online - price >0
t.test (bpp$p_diff, mu =0, alternative = 'greater')
## p  value is 99% so if we rejected the null there is 99% thats we make a false positif and do an error therefor we dont reject the nulln hypothesis
## HW : Filter to USA and price < 1000
# do two sided t-test
#Multiple hypothesis testing
testing <- bpp %>%
select( country, p_diff) %>%
group_by( country) %>%
summarize(mean_pdiff = mean(p_diff),
se_pdiff = 1/sqrt(n())*sd(p_diff),
num_obs = n())
testing
testing <- mutate(testing, t_stat = mean_pdiff/se_pdiff)
testing
testing <- mutate(testing, p_val = pt(-abs(t_stat), df = num_obs - 1))
testing
testing <- mutate(testing, p_val = round(p_val, digit = 4))
testing
###########################
## CLASS 5 - Coding in R ##
##                       ##
## Billion-Price-Project ##
###########################
#### Association
# association between online-offline prices
ggplot(bpp, aes(x= price_online, y = price)) +
geom_point(color = 'red') +
labs( x= 'price online' , y = 'price offline')+
geom_smooth(method = 'lm', formula = y ~ x)
# Bin-scatter
# 1) 'easy way' : using equal distances
ggplot( bpp, aes( x = price_online, y = price ))+
stat_summary_bin(fun='mean', binwidth = 50, color = 'red', geom = 'point',size = 2)
## group by countries
ggplot( bpp, aes( x = price_online, y = price, color = country ))+
stat_summary_bin(fun='mean', binwidth = 50, geom = 'point',size = 2)+
labs( x= 'price online' , y = 'price offline', color = 'country')+
facet_wrap(~country, scales = 'free', ncol = 2) +
theme(legend.position = 'none')+ # to remove the legend
geom_smooth(method = 'lm', formula = y~x)
ggplot(data = dp, aes(x = price_marg, fill = category_f))+
geom_histogram(binwidth = 200, col = "black")+
labs(x='Price of Margarita Pizza', y='Number of Pizzas', title = "Distribution of Price for On-site", fill= 'category')+
facet_wrap(~category_f)
# Init
rm(list=ls())
### Install and import packages
#install.packages("geosphere")
library(tidyverse)
library(geosphere)
library(moments)
library(dplyr)
my_url <- "https://raw.githubusercontent.com/Julianna-szabo/DA_team_project/master/data/raw/dp.csv"
dp <- read_csv(my_url)
## Calculate distance from CEU campus
ceu = c(47.501348, 19.049375)
dp["dist_ceu"] = distm(data.matrix(dp[6:7]), ceu, fun = distHaversine)
distm(ceu, c(47.500386, 19.049434), fun=distHaversine)
## Calculate usual number of hours open
get_interval <- function(open,close){
intv = close-open
if (intv < 0) {
intv = as.difftime("24:00:00")-open+close
}
intv=as.numeric(as.difftime(intv,units = "hours"))/60
return(intv)
}
dp$open_mins=mapply(get_interval, dp$open, dp$close)
## Data Analysis
dp_summary_stats_pizza <- summarise(dp,
n= n(),
mean = mean(x = price_marg),
median = median(x = price_marg),
min= min(price_marg),
max = max(price_marg),
sd = sd(price_marg),
skew = skewness(price_marg))
dp_summary_stats_bev <- summarise(dp,
n= n(),
mean = mean(x = price_bev),
median = median(x = price_bev),
min= min(price_bev),
max = max(price_bev),
sd = sd(price_bev),
skew = skewness(price_bev))
## Data Visulization Part 1
dp %>%
ggplot(aes(x=price_marg))+
geom_histogram(binwidth = 200, fill= "coral2", col= "black")+
theme_bw()+
labs(x = 'Price of Margarita Pizza', y= 'Number of Pizzas', title = "Distribution of Price for Pizza")
dp %>%
ggplot(aes(x=price_bev))+
geom_histogram(binwidth = 50, fill= "light blue", col= "black")+
theme_bw()+
labs(x = 'Price of 0.5L beverage', y= 'Number of Pizzas', title = "Distribution of Price for Beverage")
## Data Visualization Part 2 - to be worked on
## Creating a factor variable
dp$category_f <- factor(dp$category)
ggplot(data = dp, aes(x = price_marg, fill = category_f))+
geom_histogram(binwidth = 200, col = "black")+
labs(x='Price of Margarita Pizza', y='Number of Pizzas', title = "Distribution of Price for On-site", fill= 'category')+
facet_wrap(~category_f)
#T -test
# Hypothesis
# Null mean(on-site)=mean(delivery)
# Alternative mean(on-site) not = mean(delivery)
price_offline <- dp$price_marg[dp$category=="On-site"]
price_online <- dp$price_marg[dp$category=="Delivery"]
t.test(price_offline,price_online, alternative = "two.sided")
ggplot(data = dp, aes(x = price_marg, fill = category_f))+
geom_histogram(binwidth = 200, col = "black")+
labs(x='Price of Margarita Pizza', y='Number of Pizzas', title = "Distribution of Price for On-site", fill= 'category')+
facet_wrap(~category_f)
ggplot(data = dp, aes(x = price_marg, fill = category_f))+
geom_histogram(binwidth = 200, col = "black")+
labs(x='Price of Margarita Pizza', y='Number of Pizzas', title = "Distribution of Price for On-site", fill= 'category')
ggplot(data = dp, aes(x = price_marg, fill = category_f))+
geom_density(binwidth = 200, col = "black")+
labs(x='Price of Margarita Pizza', y='Number of Pizzas', title = "Distribution of Price for On-site", fill= 'category')
ggplot(data = dp, aes(x = price_marg, fill = category_f, alpha = 0.2))+
geom_density(binwidth = 200, col = "black")+
labs(x='Price of Margarita Pizza', y='Number of Pizzas', title = "Distribution of Price for On-site", fill= 'category')
View(dp)
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(geosphere)
library(moments)
library(dplyr)
library(knitr)
## Github Repo of Assignment: https://github.com/Julianna-szabo/DA_team_project
# Import data using paths
#setwd("C:/Users/Dominik/OneDrive - Central European University/1st_trimester/DA1/DA_team_project")
#esource="C:/Users/Dominik/OneDrive - Central European University/1st_trimester/DA1/DA_team_project"
# Import via URL
my_url <- "https://raw.githubusercontent.com/Julianna-szabo/DA_team_project/master/data/raw/dp.csv"
dp <- read_csv(my_url)
my_url <- "https://raw.githubusercontent.com/Julianna-szabo/DA_team_project/master/data/raw/dp.csv"
dp <- read_csv(my_url)
View(dp)
View(dp)
ggplot(data = dp, aes(x = price_marg, fill = category_f, alpha = 0.2))+
geom_density(binwidth = 200, col = "black")+
labs(x='Price of Margarita Pizza (HUF)', y='Number of Pizzas', title = "Distribution of Price for On-site and Delivery", fill= 'category')
# Init
rm(list=ls())
### Install and import packages
#install.packages("geosphere")
library(tidyverse)
library(geosphere)
library(moments)
library(dplyr)
my_url <- "https://raw.githubusercontent.com/Julianna-szabo/DA_team_project/master/data/raw/dp.csv"
dp <- read_csv(my_url)
## Calculate distance from CEU campus
ceu = c(47.501348, 19.049375)
dp["dist_ceu"] = distm(data.matrix(dp[6:7]), ceu, fun = distHaversine)
distm(ceu, c(47.500386, 19.049434), fun=distHaversine)
## Calculate usual number of hours open
get_interval <- function(open,close){
intv = close-open
if (intv < 0) {
intv = as.difftime("24:00:00")-open+close
}
intv=as.numeric(as.difftime(intv,units = "hours"))/60
return(intv)
}
dp$open_mins=mapply(get_interval, dp$open, dp$close)
## Data Analysis
dp_summary_stats_pizza <- summarise(dp,
n= n(),
mean = mean(x = price_marg),
median = median(x = price_marg),
min= min(price_marg),
max = max(price_marg),
sd = sd(price_marg),
skew = skewness(price_marg))
dp_summary_stats_bev <- summarise(dp,
n= n(),
mean = mean(x = price_bev),
median = median(x = price_bev),
min= min(price_bev),
max = max(price_bev),
sd = sd(price_bev),
skew = skewness(price_bev))
## Data Visulization Part 1
dp %>%
ggplot(aes(x=price_marg))+
geom_histogram(binwidth = 200, fill= "coral2", col= "black")+
theme_bw()+
labs(x = 'Price of Margarita Pizza (HUF)', y= 'Number of Pizzas', title = "Distribution of Price for Pizza")
dp %>%
ggplot(aes(x=price_bev))+
geom_histogram(binwidth = 50, fill= "light blue", col= "black")+
theme_bw()+
labs(x = 'Price of 0.5L beverage (HUF)', y= 'Number of Pizzas', title = "Distribution of Price for Beverage")
## Data Visualization Part 2
## Creating a factor variable
dp$category_f <- factor(dp$category)
ggplot(data = dp, aes(x = price_marg, fill = category_f))+
geom_histogram(binwidth = 200, col = "black")+
labs(x='Price of Margarita Pizza', y='Number of Pizzas', title = "Distribution of Price for On-site", fill= 'category')+
facet_wrap(~category_f)
ggplot(data = dp, aes(x = price_marg, fill = category_f, alpha = 0.2))+
geom_density(binwidth = 200, col = "black")+
labs(x='Price of Margarita Pizza (HUF)', y='Number of Pizzas', title = "Distribution of Price for On-site and Delivery", fill= 'category')
#T -test
# Hypothesis
# Null mean(on-site)=mean(delivery)
# Alternative mean(on-site) not = mean(delivery)
price_offline <- dp$price_marg[dp$category=="On-site"]
price_online <- dp$price_marg[dp$category=="Delivery"]
t.test(price_offline,price_online, alternative = "two.sided")
my_url <- "https://raw.githubusercontent.com/Julianna-szabo/DA_team_project/master/data/raw/dp.csv"
y_url <- "https://raw.githubusercontent.com/Julianna-szabo/DA_team_project/master/data/raw/dp.csv"
dp <- read_csv(my_url)
ggplot(data = dp, aes(x = price_marg, fill = category_f, alpha = 0.2))+
geom_density(binwidth = 200, col = "black")+
labs(x='Price of Margarita Pizza (HUF)', y='Number of Pizzas', title = "Distribution of Price for On-site and Delivery", fill= 'category')
ggplot(data = dp, aes(x = price_marg, fill = category_f, alpha = 0.2))+
geom_density(binwidth = 200, col = "black")+
labs(x='Price of Margarita Pizza (HUF)', y='Number of Pizzas', title = "Distribution of Price for On-site and Delivery", fill= 'category')
ggplot(data = dp, aes(x = price_marg, fill = category_f))+
geom_histogram(binwidth = 200, col = "black")+
labs(x='Price of Margarita Pizza', y='Number of Pizzas', title = "Distribution of Price for On-site", fill= 'category')+
facet_wrap(~category_f)
library(tidyverse)
library(geosphere)
library(moments)
library(dplyr)
ggplot(data = dp, aes(x = price_marg, fill = category_f, alpha = 0.2))+
geom_density(binwidth = 200, col = "black")+
labs(x='Price of Margarita Pizza (HUF)', y='Number of Pizzas', title = "Distribution of Price for On-site and Delivery", fill= 'category')
# Init
rm(list=ls())
### Install and import packages
#install.packages("geosphere")
library(tidyverse)
library(geosphere)
library(moments)
library(dplyr)
my_url <- "https://raw.githubusercontent.com/Julianna-szabo/DA_team_project/master/data/raw/dp.csv"
dp <- read_csv(my_url)
## Calculate distance from CEU campus
ceu = c(47.501348, 19.049375)
dp["dist_ceu"] = distm(data.matrix(dp[6:7]), ceu, fun = distHaversine)
distm(ceu, c(47.500386, 19.049434), fun=distHaversine)
## Calculate usual number of hours open
get_interval <- function(open,close){
intv = close-open
if (intv < 0) {
intv = as.difftime("24:00:00")-open+close
}
intv=as.numeric(as.difftime(intv,units = "hours"))/60
return(intv)
}
dp$open_mins=mapply(get_interval, dp$open, dp$close)
## Data Analysis
dp_summary_stats_pizza <- summarise(dp,
n= n(),
mean = mean(x = price_marg),
median = median(x = price_marg),
min= min(price_marg),
max = max(price_marg),
sd = sd(price_marg),
skew = skewness(price_marg))
dp_summary_stats_bev <- summarise(dp,
n= n(),
mean = mean(x = price_bev),
median = median(x = price_bev),
min= min(price_bev),
max = max(price_bev),
sd = sd(price_bev),
skew = skewness(price_bev))
## Data Visulization Part 1
dp %>%
ggplot(aes(x=price_marg))+
geom_histogram(binwidth = 200, fill= "coral2", col= "black")+
theme_bw()+
labs(x = 'Price of Margarita Pizza (HUF)', y= 'Number of Pizzas', title = "Distribution of Price for Pizza")
dp %>%
ggplot(aes(x=price_bev))+
geom_histogram(binwidth = 50, fill= "light blue", col= "black")+
theme_bw()+
labs(x = 'Price of 0.5L beverage (HUF)', y= 'Number of Pizzas', title = "Distribution of Price for Beverage")
## Data Visualization Part 2
## Creating a factor variable
dp$category_f <- factor(dp$category)
ggplot(data = dp, aes(x = price_marg, fill = category_f))+
geom_histogram(binwidth = 200, col = "black")+
labs(x='Price of Margarita Pizza', y='Number of Pizzas', title = "Distribution of Price for On-site", fill= 'category')+
facet_wrap(~category_f)
ggplot(data = dp, aes(x = price_marg, fill = category_f, alpha = 0.2))+
geom_density(binwidth = 200, col = "black")+
labs(x='Price of Margarita Pizza (HUF)', y='Number of Pizzas', title = "Distribution of Price for On-site and Delivery", fill= 'category')
#T -test
# Hypothesis
# Null mean(on-site)=mean(delivery)
# Alternative mean(on-site) not = mean(delivery)
price_offline <- dp$price_marg[dp$category=="On-site"]
price_online <- dp$price_marg[dp$category=="Delivery"]
t.test(price_offline,price_online, alternative = "two.sided")
ggplot(data = dp, aes(x = price_marg, fill = category_f))+
geom_histogram(binwidth = 200, col = "black")+
labs(x='Price of Margarita Pizza', y='Number of Pizzas', title = "Distribution of Price for On-site", fill= 'category')+
facet_wrap(~category_f)
ggplot(data = dp, aes(x = price_marg, fill = category_f, alpha = 0.2))+
geom_density(binwidth = 200, col = "black")+
labs(x='Price of Margarita Pizza (HUF)', y='Number of Pizzas', title = "Distribution of Price for On-site and Delivery", fill= 'category')
